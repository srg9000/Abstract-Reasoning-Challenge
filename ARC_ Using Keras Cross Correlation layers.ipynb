{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Change Tf and keras version because that's what the layer was written in","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow==1.13.1\n!pip install keras==2.1.2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import necessary libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nimport cv2\nfrom sklearn.utils import shuffle\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining the layer\nThis has been taken from https://software.intel.com/content/www/us/en/develop/articles/keras-implementation-of-siamese-like-networks.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Input, Flatten\nfrom keras.models import Model, Sequential\nfrom keras.engine import InputSpec, Layer\nfrom keras import regularizers\nfrom keras.optimizers import SGD, Adam\nfrom keras.utils.conv_utils import conv_output_length\nfrom keras import activations\nimport numpy as np\n\n\nclass Normalized_Correlation_Layer(Layer):\n\n# create a class inherited from keras.engine.Layer.\n\n    def __init__(self, patch_size=(5, 5),\n          dim_ordering='tf',\n          border_mode='same',\n          stride=(1, 1),\n          activation=None,\n          **kwargs):\n\n       if border_mode != 'same': \n          raise ValueError('Invalid border mode for Correlation Layer ' \n                     '(only \"same\" is supported as of now):', border_mode) \n       self.kernel_size = patch_size \n       self.subsample = stride \n       self.dim_ordering = dim_ordering \n       self.border_mode = border_mode \n       self.activation = activations.get(activation) \n       super(Normalized_Correlation_Layer, self).__init__(**kwargs)\n\n    def compute_output_shape(self, input_shape):\n        return(input_shape[0][0], input_shape[0][1], input_shape[0][2], self.kernel_size[0] * input_shape[0][2]*input_shape[0][-1])\n\n    def get_config(self): \n        config = {'patch_size': self.kernel_size, \n              'activation': self.activation.__name__, \n              'border_mode': self.border_mode, \n              'stride': self.subsample, \n              'dim_ordering': self.dim_ordering} \n        base_config = super(Correlation_Layer, self).get_config() \n        return dict(list(base_config.items()) + list(config.items()))\n\n    def call(self, x, mask=None):\n        input_1, input_2 = x\n        stride_row, stride_col = self.subsample\n        inp_shape = input_1._keras_shape\n        output_shape = self.compute_output_shape([inp_shape, inp_shape])\n        padding_row = (int(self.kernel_size[0] / 2),int(self.kernel_size[0] / 2)) \n        padding_col = (int(self.kernel_size[1] / 2),int(self.kernel_size[1] / 2)) \n        input_1 = K.spatial_2d_padding(input_1, padding =(padding_row,padding_col)) \n        input_2 = K.spatial_2d_padding(input_2, padding = ((padding_row[0]*2, padding_row[1]*2),padding_col)) \n        output_row = output_shape[1] \n        output_col = output_shape[2] \n        output = [] \n        for k in range(inp_shape[-1]):\n               xc_1 = [] \n               xc_2 = [] \n            #    print(\"here\")\n               for i in range(padding_row[0]): \n                  for j in range(output_col): \n                     xc_2.append(K.reshape(input_2[:, i:i+self.kernel_size[0], j:j+self.kernel_size[1], k], \n                                 (-1, 1,self.kernel_size[0]*self.kernel_size[1])))\n\n        for i in range(output_row): \n            slice_row = slice(i, i + self.kernel_size[0]) \n            slice_row2 = slice(i + padding_row[0], i +self.kernel_size[0] + padding_row[0]) \n            # print(\"dfg\")\n            for j in range(output_col): \n                slice_col = slice(j, j + self.kernel_size[1]) \n                xc_2.append(K.reshape(input_2[:, slice_row2, slice_col, k], \n                           (-1, 1,self.kernel_size[0]*self.kernel_size[1]))) \n                xc_1.append(K.reshape(input_1[:, slice_row, slice_col, k], \n                             (-1, 1,self.kernel_size[0]*self.kernel_size[1])))\n        for i in range(output_row, output_row+padding_row[1]): \n            for j in range(output_col): \n                xc_2.append(K.reshape(input_2[:, i:i+ self.kernel_size[0], j:j+self.kernel_size[1], k], \n                           (-1, 1,self.kernel_size[0]*self.kernel_size[1]))) \n\n        xc_1_aggregate = K.concatenate(xc_1, axis=1)\n        xc_1_mean = K.mean(xc_1_aggregate, axis=-1, keepdims=True) \n        xc_1_std = K.std(xc_1_aggregate, axis=-1, keepdims=True) \n        xc_1_aggregate = (xc_1_aggregate - xc_1_mean) / xc_1_std\n\n        xc_2_aggregate = K.concatenate(xc_2, axis=1)\n        xc_2_mean = K.mean(xc_2_aggregate, axis=-1, keepdims=True) \n        xc_2_std = K.std(xc_2_aggregate, axis=-1, keepdims=True) \n        xc_2_aggregate = (xc_2_aggregate - xc_2_mean) / xc_2_std\n\n        xc_1_aggregate = K.permute_dimensions(xc_1_aggregate, (0, 2, 1)) \n        block = [] \n        len_xc_1= len(xc_1) \n        print(\"asdf\")\n        for i in range(len_xc_1):\n                 #This for loop is to compute the product of a given patch of feature map 1 and the feature maps on which it is supposed to\n            sl1 = slice(int(i/inp_shape[2])*inp_shape[2], \n                  int(i/inp_shape[2])*inp_shape[2]+inp_shape[2]*self.kernel_size[0]) \n                      #This calculates which are the patches of feature map 2 to be considered for a given patch of first feature map.\n\n            block.append(K.reshape(K.batch_dot(xc_2_aggregate[:,sl1,:], \n                          xc_1_aggregate[:,:,i]),(-1,1,1,inp_shape[2] *self.kernel_size[0]))) \n\n        block = K.concatenate(block, axis=1) \n        # print(\"zxcv\")\n        block= K.reshape(block,(-1,output_row,output_col,inp_shape[2] *self.kernel_size[0])) \n        output.append(block)\n\n        output = self.activation(output) \n        try:\n            print(output.numpy().shape)\n        except:\n            pass\n        return output\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/input/abstraction-and-reasoning-challenge","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data and save to dictionaries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/input/abstraction-and-reasoning-challenge')\nfull_data = []\ndef read_files(dir_):\n    file_data = {}\n    file_data_train = []\n    file_names = []\n    for file in list(os.listdir(dir_)):\n        with open(dir_ + '/' + file) as f:\n\n            fd = json.loads(f.read())\n            if dir_ != 'test':\n                file_data[file] = {\n                \t\t\t\t \"train\":{ \"ip\":[i['input'] for i in fd['train']], \n                \t\t\t\t \t\t   \"op\": [i['output'] for i in fd['train']]},\n\n                \t\t\t\t \"test\":{ \"ip\":[i['input'] for i in fd['test']], \n                \t\t\t\t \t\t  \"op\": [i['output'] for i in fd['test']]}\n                \t\t\t\t }\n                file_names.append(file)\n            else:\n                file_data[file] = {\n                \t\t\t\t \"train\":{ \"ip\":[i['input'] for i in fd['train']], \n                \t\t\t\t \t\t\t\"op\": [i['output'] for i in fd['train']]},\n\n                \t\t\t\t \"test\":{\"ip\":[i['input'] for i in fd['test']], \n                \t\t\t\t \"op\": []}\n                \t\t\t\t }\n                file_names.append(file)\n    full_data.append(file_data)\n\n    with open('/kaggle/working'+dir_ + '_file_data2.pickle', 'wb') as f:\n        pickle.dump(file_data, f)\n    with open('/kaggle/working'+dir_ + '_data2.pickle', 'wb') as f:\n        pickle.dump([file_data, file_names], f)\n\n    return file_data, file_names\n\n\nfor i in ['training','test','evaluation']:\n    if os.path.isdir(i):\n        x = read_files(i)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dict, val_dict, test_dict = full_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print full arrays without ... in between\ndef fullprint(*args, **kwargs):\n  from pprint import pprint\n  import numpy\n  opt = numpy.get_printoptions()\n  numpy.set_printoptions(threshold=numpy.inf)\n  pprint(*args, **kwargs)\n  numpy.set_printoptions(**opt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Padding all arrays to keep the sizes same (50x50)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_arr = np.zeros((50,50),dtype=np.int8) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Superimpose inputs and outputs to the center of padding array\nOriginal values are increased by 1 because the padding is zero and so all original zero values should be maintained","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def center_merge(a,b):\n#     print(a.shape,b.shape)\n    x1 = a.shape[1]//2 - b.shape[1]//2\n    y1 = a.shape[0]//2 - b.shape[0]//2\n    x2 = a.shape[1]//2 - b.shape[1]//2 + b.shape[1]\n    y2 = a.shape[0]//2 - b.shape[0]//2 + b.shape[0]\n#     try:\n    a[y1:y2, x1:x2] = b+1   # b+1 because the padding is zero and so all original zero values should be maintained\n#     except:\n    a = np.reshape(a, (50,50))\n    \n    return a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pad all arrays in train, validation and test set\nfor dicts in [train_dict, val_dict, test_dict]:\n    for dct in dicts:\n        for setval in dicts[dct].keys():\n            for iop in dicts[dct][setval].keys():\n                for i,mat in enumerate(dicts[dct][setval][iop]):\n                    dicts[dct][setval][iop][i] = center_merge(pad_arr.copy(),np.array(mat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Changing colours in inputs and outputs \nThis generates permutations for mapping original colour to other colours in the range","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mappings():\n    l = np.array([i for i in range(2,12)])\n    shuffled = []\n    for i in range(50):\n        sh = l.copy()\n        np.random.shuffle(sh)\n        shuffled.append(sh)\n    mappings = [dict(zip(l,i)) for i in shuffled]\n    return mappings\n\n# mappings = create_mappings()\n\ndef create_permutations(a,dicts):\n    permuted = []\n    for d in dicts:\n        b = a.copy()\n        for k, v in d.items():\n            b[a==k] = v\n        b = np.array(b)\n        permuted.append(np.reshape(np.array(b).astype(np.uint8),(50,50)))\n    permuted = np.reshape(np.array(permuted),(-1,50,50))\n    return permuted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## I previously intended to create masks of nxm dimension and input those as different channels after convolving\nTakes a lot of memory to do so, so it is not used further","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_masks(orig):\n    kernels = [np.ones((1,1),dtype=np.uint8),np.ones((2,1),dtype=np.uint8),\n               np.ones((1,2),dtype=np.uint8),np.ones((3,1),dtype=np.uint8),\n               np.ones((1,3),dtype=np.uint8),np.ones((3,3),dtype=np.uint8)]\n    arr = orig.copy()\n    masked = np.array(arr,dtype=np.uint8)\n    for kernel in kernels:\n        mask = np.zeros((50,50),dtype=np.uint8)\n        mask[:kernel.shape[0],:kernel.shape[1]] = kernel\n        mask_orig = mask.copy()\n        mask = np.roll(mask,1)\n        print(mask.all()!=mask_orig.all())\n        counter=0\n        while not mask.all()!=mask_orig.all():\n            mask = np.roll(mask,1)\n            print(counter)\n            counter+=1\n            arr_mask = np.bitwise_and(arr,mask)\n            if arr_mask.all() == np.zeros((50,50)).all():\n                continue\n            masked = np.append(masked,arr_mask, axis=0)\n            if mask[-1,-1]==1:\n                print(counter,'\\n',mask)\n                break\n    return masked\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Convert all inputs/outputs to onehot maps, ie one channel for every value in 0-11","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def expand_op(op):\n    op = np.reshape(np.array(op),(50,50))\n    op_arr = []\n    l = np.array([i for i in range(0,12)])\n    for i in l:\n        op_arr.append(np.reshape(np.array([op==i]).astype(np.uint8), (50,50)))\n    return np.reshape(np.array(op_arr).astype(np.uint8), (50,50,12))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply all transformations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_data(data_dict):\n    for task in sorted(list(data_dict.keys())):\n        ip_perm = []\n        op_perm = []\n#         print(np.array(data_dict[task]['train']['ip']).shape)\n        for i in range(len(data_dict[task]['train']['ip'])):\n            mappings = create_mappings()\n            \n            ip_perm.extend(create_permutations(data_dict[task]['train']['ip'][i],mappings))\n            op_perm.extend(create_permutations(data_dict[task]['train']['op'][i],mappings))\n        data_dict[task]['train']['ip'].extend(ip_perm)\n#         print(np.array(data_dict[task]['train']['ip']).shape)\n        data_dict[task]['train']['op'].extend(op_perm)\n        data_dict[task]['train']['ip'],data_dict[task]['train']['op'] = shuffle(data_dict[task]['train']['ip'],data_dict[task]['train']['op'],random_state=69)\n        \n        data_dict[task]['train']['ip'] = [expand_op(i) for i in data_dict[task]['train']['ip']]\n        data_dict[task]['train']['op'] = [expand_op(i) for i in data_dict[task]['train']['op']]\n#         np.random.shuffle(data_dict[task]['train']['ip'], seed=69)\n#         np.random.shuffle(data_dict[task]['train']['op'], seed=69)\n    return data_dict\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dict = generate_data(train_dict)\n# val_dict = generate_data(val_dict)\n# test_dict = generate_data(test_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The model:\n## The model consumes a lot of memory so only a few layers are kept","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = 'float32'\ndef create_model():\n    ip = keras.layers.Input((50,50, 12))\n    ncx1_1 =  Normalized_Correlation_Layer(patch_size=(1, 1))([ip,ip])\n    ncx1_1 = keras.layers.Reshape((50,50,600))(ncx1_1)\n    ncn1_1 = keras.layers.Conv2D(64, (1,1), activation = 'relu', dtype=dt)(ip)\n    ncn2_1 = keras.layers.Conv2D(64, (1,1), activation = 'relu', dtype=dt)(ncx1_1)\n    ncx2_1 =  Normalized_Correlation_Layer(patch_size=(1, 1),dtype=dt)([ncn1_1,ncn2_1])\n    ncn3 = keras.layers.LocallyConnected2D(filters=64,kernel_size=(1,1), activation = 'relu', dtype=dt)(ncx2_1)\n    ncn4 = keras.layers.Conv2D(12, (1,1), activation = 'sigmoid', dtype=dt)(ncn3)\n    \n    model = keras.models.Model(ip,ncn4)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"md = create_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"md.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"md.compile(optimizer = 'adam', loss = 'categorical_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A custom generator that generates data as per task, keeping all tasks in same batch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Custom_Generator(keras.utils.Sequence) :\n    def __init__(self, data_dict, batch_size=50, prps='train') :\n        self.data_dict = data_dict\n        self.tg = self.train_generator()\n        self.prps = prps\n    def __len__(self) :\n        x = 0\n        for task in sorted(list(self.data_dict.keys())):\n            x += len(self.data_dict[task][self.prps]['ip'])\n        print(\"len = \",np.ceil(x/50).astype(np.int))\n        return np.ceil(x/50).astype(np.int)\n    \n    def train_generator(self):\n        for task in sorted(list(self.data_dict.keys())):\n            x = np.array(self.data_dict[task][self.prps]['ip'])\n            x = np.reshape(x, (-1,50,50,12))\n            y = self.data_dict[task][self.prps]['op']\n            y = np.reshape(y, (-1,50,50,12))\n#             print(x.shape)\n            for i in range(0,len(x),50):\n                yield (np.array([x[i:i+50]]), np.array([y[i:i+50]]))\n            \n    def __getitem__(self, idx) :\n        \n        batch_x, batch_y = next(self.tg)\n        print(idx, batch_x.shape,batch_y.shape)\n        return ( np.reshape(batch_x,(-1,50,50,12)), np.reshape(batch_y,(-1,50,50,12)) )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = Custom_Generator(train_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Does not fit due to memory constraints, speed/no of operations etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"md.fit_generator(train_gen,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from numpy import copy, random, arange\n\n# random.seed(0)\n# data = random.randint(30, size=10**5)\n\n# d = {4: 0, 9: 5, 14: 10, 19: 15, 20: 0, 21: 1, 22: 2, 23: 3, 24: 0}\n# dk = d.keys()\n# dv = d.values()\n\n# def f1(a, d):\n#     b = copy(a)\n#     for k, v in d.items():\n#         b[a==k] = v\n#     return b\n\n# def f2(a, d):\n#     for i in range(len(a)):\n#         a[i] = d.get(a[i], a[i])\n#     return a\n\n# dget = np.vectorize(d.get)\n# def f4(a,d):\n#     return dget(a)\n\n\n# a = copy(data)\n# res = f2(a, d)\n\n# # assert (f1(data, d) == res).all()\n# # assert (f3(data, dk, dv) == res).all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %timeit f1(data,d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %timeit f4(data,d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}